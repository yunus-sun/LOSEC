# LOSEC: Local Semantic Capture Empowered Large Time Series Model for IoT-Enabled Data Centers

Deep learning methods for accurately predicting data center status have gained significant attention with the advancement of Internet of Things (IoT) technologies, which make the collection of massive data possible. This capability is crucial for mitigating the exponential growth of energy consumption. However, conventional small models often face data scarcity issues in practical deployment. While large models show promise in addressing this challenge, they encounter obstacles such as multivariate tasks, computational intensity, and ineffective information capture. Moreover, their applications in data centers remain largely unexplored. In this paper, we investigate local semantic capture empowered large model for multivariate time series forecasting in IoT-enabled data centers. We first introduce time series tasks within data centers and propose the Point Lag (Plag)-Llama framework with the Lag-Llama backbone to support zero-shot forecasting and fine-tuning for multivariate point time series forecasting. To address computational intensity and enhance the capabilities of multivariate forecasting, we propose the \textbf{Lo}cal \textbf{Se}mantic \textbf{C}apture (LOSEC) for adapter fine-tuning, which captures local semantic information across time and channel dimensions alternately with low-complexity. Specifically, time series are segmented into tokens, and channels are clustered together, forming local semantic information that can be captured more effectively. Extensive experiments demonstrate that Plag-Llama exhibits superior zero-shot capability and that the LOSEC empowered adapter fine-tuning achieves state-of-the-art performance on real-world datasets collected from data centers, with ablation studies further validating the effectiveness of each module within the proposed models.

**This work has been submitted for peer review.**
